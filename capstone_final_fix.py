# -*- coding: utf-8 -*-
"""Capstone Final FIX

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17wvkwB4yp_C7YEodzdcDQQrYsGtsIfW1

#**OVERVIEW**
- Menentukan interest category
- Menilai technical skill (berdasarkan MCQ)
- Menentukan level user (Dasar â†’ Profesional)
- Menentukan learning path berdasarkan kategori dan skor
- Mengambil roadmap modul sesuai learning path & level dari JSON


**Backend hanya perlu memanggil fungsi phyton berikut:**

from chatbot_pipeline import chatbot_pipeline

# **Install dan Import Library**
"""

!pip install requests pandas scikit-learn fastapi uvicorn python-multipart joblib

!pip install gdown

import requests
import numpy as np
import pandas as pd
import seaborn as sns
import os
import gdown
import re
import ast
import json

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""# **Load Data**"""

BASE = "https://jrkqcbmjknzgpbtrupxh.supabase.co/rest/v1/"
HEADERS = {"apikey": "sb_publishable_h889CjrPIGwCMA9I4oTTaA_2L22Y__R",
           "Authorization": "Bearer sb_publishable_h889CjrPIGwCMA9I4oTTaA_2L22Y__R"}

ENDPOINTS = {
    "learning_paths": "learning_paths",
    "courses": "courses",
    "course_levels": "course_levels",
    "tutorials": "tutorials"
}

os.makedirs("../data", exist_ok=True)

def fetch_and_save(name, endpoint):
    url = BASE + endpoint
    r = requests.get(url, headers=HEADERS)
    r.raise_for_status()
    data = r.json()
    df = pd.DataFrame(data)
    df.to_csv(f"../data/{name}.csv", index=False)
    print(f"Saved ../data/{name}.csv ({len(df)} rows)")

if __name__ == "__main__":
    for name, ep in ENDPOINTS.items():
        fetch_and_save(name, ep)

def load_from_drive(url, filename):
    file_id = url.split('/d/')[1].split('/')[0]
    gdown.download(f"https://drive.google.com/uc?id={file_id}", filename, quiet=False)
    return pd.read_csv(filename)

learning_path_course = load_from_drive(
    "https://drive.google.com/file/d/1_YNT_irFtmwmcCGaq-dVvclFDRgIf86s/view?usp=sharing",
    "lp_course.csv"
)

learning_path = load_from_drive(
    "https://drive.google.com/file/d/1p9vuDJIpfsgm21NILaus-hVwxgFSSNci/view?usp=sharing",
    "learning_paths.csv"
)

course = load_from_drive(
    "https://drive.google.com/file/d/1WF0u7WnvhB8Yfu_5kICOWvwi_LuaQgY4/view?usp=sharing",
    "course.csv"
)

tutorials = load_from_drive(
    "https://drive.google.com/file/d/1pof_qubNBe_JhhQGM4yW4hhze7CW-xJI/view?usp=sharing",
    "tutorials.csv"
)

course_level = load_from_drive(
    "https://drive.google.com/file/d/1Yue2WaGQyexCvEH3Qw-7cMwUZTBo9KPm/view?usp=sharing",
    "course_levels.csv"
)

learning_path_answer = load_from_drive(
    "https://drive.google.com/file/d/1Vu1ZBR6ERECVweetpFT5QLTosrVVoQhG/view?usp=sharing",
    "learning_path_answer.csv"
)

current_interest_question = load_from_drive(
    "https://drive.google.com/file/d/10YxCuufKQSbk43ptFpOWINODZOal1zAy/view?usp=sharing",
    "current_interest_question.csv"
)

current_tech_question = load_from_drive(
    "https://drive.google.com/file/d/1vP_f61S7c2Ujj0JSHLVYkQvmfiQ-cO1q/view?usp=sharing",
    "current_tech_question.csv"
)

skill_keywords = load_from_drive(
    "https://drive.google.com/file/d/1Vu1ZBR6ERECVweetpFT5QLTosrVVoQhG/view?usp=sharing",
    "skill_keywords.csv"
)

student_progress = load_from_drive(
    "https://drive.google.com/file/d/1CmenRPmELmMAc23myCDDtIALmnq1FMwI/view?usp=sharing",
    "student_progress.csv"
)

def load_from_drive(file_id: str):

    url = f"https://drive.google.com/uc?export=download&id={file_id}"
    response = requests.get(url)

    if response.status_code == 200:
        return json.loads(response.text)
    else:
        raise Exception(f"Download gagal, status code: {response.status_code}")

# Gunakan file_id dari link Google Drive
file_id = "11Ls5dz8-acaOqo5-aA-rfDgDLomvOkNu"
roadmap = load_from_drive(file_id)

print(roadmap)  # cek isi JSON

df_category = pd.read_csv('/content/current_tech_question.csv')

df_category['tech_category'].unique()

"""# **Preprocessing Data**"""

student_progress.head()

student_progress.tail()

student_progress.describe()

student_progress.dtypes

student_progress.size

student_progress.columns

student_progress.duplicated().sum()

student_progress.isnull().sum()

sns.heatmap(student_progress.isnull())

#fitur biner dari kolom ID
student_progress['has_submission'] = student_progress['final_submission_id'].notna().astype(int) # 1 = user punya submission 0 = user belum submit
student_progress['has_final_exam'] = student_progress['final_exam_id'].notna().astype(int) #
student_progress = student_progress.drop(['final_submission_id', 'final_exam_id'], axis=1)

#kalau 0 = belum submit / belum ikut ujian
student_progress['submission_rating'] = student_progress['submission_rating'].fillna(0)
student_progress['exam_score'] = student_progress['exam_score'].fillna(0)

student_progress.isnull().sum()

sns.heatmap(student_progress.isnull())

print("learning_path_course")
display(learning_path_course.head())

print("learning_path")
display(learning_path.head())

print("course")
display(course.head())

print("tutorials")
display(tutorials.head())

print("course_level")
display(course_level.head())

print("learning_path_answer")
display(learning_path_answer.head())

print("current_interest_question")
display(current_interest_question.head())

print("current_tech_question")
display(current_tech_question.head())

print("skill_keywords")
display(skill_keywords.head())

print("student_progress")
display(student_progress.head())

# Membersihkan kolom teks dari HTML & whitespace
def clean_html_text(s):
    if pd.isna(s):
        return ""
    s = str(s)
    s = s.replace("&nbsp;", " ")
    s = re.sub(r'\s+', ' ', s)
    s = re.sub(r'<[^>]+>', '', s)
    return s.strip()

# Terapkan ke beberapa tabel
for df in [learning_path_course, learning_path_answer, course]:
    if 'description' in df.columns:
        df['description_clean'] = df['description'].apply(clean_html_text)
    if 'summary' in df.columns:
        df['summary_clean'] = df['summary'].apply(clean_html_text)

# Normalisasi kolom technologies menjadi list token
def parse_technologies(x):
    if pd.isna(x):
        return []
    if isinstance(x, list):
        return [str(t).strip().lower() for t in x if str(t).strip()]
    s = str(x)
    parts = [p.strip().lower() for p in s.split(',') if p.strip()]
    return parts

if 'technologies' in learning_path_answer.columns:
    learning_path_answer['technologies_parsed'] = learning_path_answer['technologies'].apply(parse_technologies)

# Parsing courseMeta / courseInfo (string list)
def try_ast_literal(x):
    try:
        return ast.literal_eval(x)
    except:
        return None

def extract_meta(meta_str):
    hasil = {}
    if pd.isna(meta_str):
        return hasil

    parsed = try_ast_literal(meta_str)

    # Jika berhasil di-parse dan berupa list
    if isinstance(parsed, (list, tuple)) and len(parsed) > 0:
        for item in parsed:
            s = str(item)

            # cari jam belajar (misal: "140 Jam")
            if 'jam' in s.lower() or re.search(r'\d+\s*jam', s.lower()):
                hasil['hours'] = re.sub(r'[^0-9]', '', s)

            # cari rating format "4,84"
            elif re.match(r'^\s*\d+,\d+\s*$', s):
                hasil['rating'] = float(s.replace(',', '.'))

            # cari level text
            elif any(x in s.lower() for x in ['dasar','pemula','menengah','mahir','profesional']):
                hasil['level_text'] = s

    return hasil

if 'courseMeta' in learning_path_answer.columns:
    meta_parsed = learning_path_answer['courseMeta'].apply(extract_meta)
    learning_path_answer['meta_hours'] = meta_parsed.apply(lambda d: d.get('hours', np.nan))
    learning_path_answer['meta_rating'] = meta_parsed.apply(lambda d: d.get('rating', np.nan))
    learning_path_answer['meta_level_text'] = meta_parsed.apply(lambda d: d.get('level_text', np.nan))

# hapus kolom yang kosong di tutorials
tutorials = tutorials.loc[:, ~tutorials.columns.str.contains("^Unnamed")]

# Normalisasi level course (angka â†’ teks) menggunakan tabel course_level
level_map = {}

if 'id' in course_level.columns and 'course_level' in course_level.columns:
    for _, row in course_level.iterrows():
        level_map[int(row['id'])] = row['course_level']
# Terapkan mapping
if 'course_level_str' in course.columns:
    def map_level(x):
        try:
            idx = int(x)
            return level_map.get(idx, str(x))
        except:
            return str(x)
    course['course_level_norm'] = course['course_level_str'].apply(map_level)

print("Contoh hasil cleaning:")
display(learning_path_answer[['id','name','description_clean','technologies_parsed','meta_hours','meta_rating']].head())
display(course[['course_id','course_name','course_level_str','course_level_norm']].head())
display(tutorials.head())

"""**Cek Validitas Relasi / Foreign Key**"""

print("Jumlah course:", len(course))
print("Jumlah tutorials:", len(tutorials))

# Cek apakah tutorial mengacu ke course_id yang tidak ada
tutorial_missing = set(tutorials['course_id'].unique()) - set(course['course_id'].unique())
print("course_id yang dirujuk tutorial tapi tidak ada di tabel course:", tutorial_missing)

# Cek learning_path_id yang hilang
lp_missing = set(course['learning_path_id'].unique()) - set(learning_path['learning_path_id'].unique())
print("learning_path_id yang tidak ditemukan di tabel learning_path:", lp_missing)

# Cek duplikasi primary key
print("Duplikasi course_id:", course['course_id'].duplicated().sum())
print("Duplikasi tutorial_id:", tutorials['tutorial_id'].duplicated().sum())

"""GROUPING"""

category_map = {
    # Mobile
    "android": "Mobile Development",
    "ios": "Mobile Development",
    "multi platform": "Mobile Development",
    "multiplatform": "Mobile Development",

    # AI
    "machine learning": "Artificial Intelligence",
    "ml": "Artificial Intelligence",
    "data": "Artificial Intelligence",  # karena interest category tidak punya Data Science

    # Web
    "web": "Web Development",

    # Cloud
    "cloud": "Cloud Computing",
    "cloud computing": "Cloud Computing"
}

#Mengubah kategori original â†’ unified category
def normalize_category(cat):
    if pd.isna(cat):
        return None
    cat = str(cat).strip().lower()
    return category_map.get(cat, None)

#GROUPING: Current Interest Question
def group_interest(interest_df):
    # Normalisasi kategori (jaga-jaga kalau ada typo / inconsistent)
    interest_df['category_norm'] = interest_df['category'].apply(normalize_category)

    # Hitung skor preferensi kategori
    interest_score = interest_df['category_norm'].value_counts().reset_index()
    interest_score.columns = ['category', 'score']

    return interest_score

level_score = {
    "beginner": 1,
    "intermediate": 2,
    "advanced": 3
}

def group_tech(tech_df):
    # Normalize category
    tech_df['category_norm'] = tech_df['category'].apply(normalize_category)

    # Skor level
    tech_df['level_score'] = tech_df['level'].apply(lambda x: level_score.get(str(x).lower(), 0))

    # Agregasi skor
    tech_score = tech_df.groupby('category_norm')['level_score'].sum().reset_index()
    tech_score.columns = ['category', 'score']

    return tech_score

def merge_interest_tech(interest_score, tech_score, w_interest=0.6, w_tech=0.4):

    merged = pd.merge(interest_score, tech_score, on='category', how='outer', suffixes=('_interest', '_tech'))
    merged = merged.fillna(0)

    merged['final_score'] = merged['score_interest'] * w_interest + merged['score_tech'] * w_tech

    merged = merged.sort_values('final_score', ascending=False)

    return merged

def get_top_category(merged_df):
    return merged_df.iloc[0]['category']

def to_api_response(merged):
    result = merged.to_dict(orient='records')
    return {
        "ranking": result,
        "recommended_category": result[0]['category']
    }

"""# **Modelling**"""

import json
import pandas as pd
from collections import defaultdict, Counter, deque

PATH_INTEREST_Q = "current_interest_question.csv"
PATH_TECH_Q = "current_tech_question.csv"
PATH_MODULES_JSON = "roadmap.json"

"""**Mapping Category ke Learning Path**"""

interest_to_lp = {
    "Mobile Development": [
        "Android Developer",
        "iOS Developer",
        "Multi-Platform App Developer"
    ],
    "Artificial Intelligence": [
        "AI Engineer",
        "Gen AI Engineer",
        "Data Scientist",
        "MLOps Engineer"
    ],
    "Cloud Computing": [
        "Google Cloud Professional",
        "DevOps Engineer",
        "MLOps Engineer"
    ],
    "Web Development": [
        "Front-End Web Developer",
        "React Developer",
        "Back-End Developer Python",
        "Back-End Developer JavaScript"
    ]
}

tech_to_lp = {
    "Android": ["Android Developer"],
    "iOS": ["iOS Developer"],
    "Multi Platform": ["Multi-Platform App Developer"],
    "Web": [
        "Front-End Web Developer",
        "React Developer",
        "Back-End Developer Python",
        "Back-End Developer JavaScript"
    ],
    "Cloud Computing": [
        "Google Cloud Professional",
        "DevOps Engineer",
        "MLOps Engineer"
    ],
    "Machine Learning": [
        "AI Engineer",
        "Gen AI Engineer",
        "Data Scientist",
        "MLOps Engineer"
    ],
    "Data": ["Data Scientist"]
}

"""**Load Files**"""

def load_csv(path):
    return pd.read_csv(path)

def load_modules(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

"""**Menentukan Interest Category**"""

def determine_interest_category(interest_q_df, user_interest_answers):
    opt_to_cat = dict(zip(interest_q_df["option_text"], interest_q_df["category"]))
    scores = defaultdict(int)
    for ans in user_interest_answers:
        cat = opt_to_cat.get(ans)
        if cat:
            scores[cat] += 1
    if not scores:
        return None, {}
    best = max(scores, key=scores.get)
    return best, dict(scores)

"""**Evaluasi Tech category**"""

def evaluate_tech_mcq(tech_q_df, user_tech_answers):
    tech_scores = defaultdict(int)
    correct_count = 0
    total_answered = 0

    for _, row in tech_q_df.iterrows():
        q = row["question_desc"]
        tech_cat = row.get("tech_category", "Unknown")
        if q in user_tech_answers:
            chosen = str(user_tech_answers[q]).strip()
            total_answered += 1
            if chosen and chosen == str(row["correct_answer"]).strip():
                tech_scores[tech_cat] += 1
                correct_count += 1
            else:
                tech_scores[tech_cat] += 0

    percent_correct = (correct_count / total_answered * 100) if total_answered > 0 else None
    ranked = sorted(tech_scores.items(), key=lambda x: x[1], reverse=True)
    categories_ranked = [t for t, s in ranked]
    return categories_ranked, dict(tech_scores), percent_correct

# Mapping persentase benar ke 5-level JSON

def determine_json_level_from_percent(percent_correct):
    if percent_correct is None:
        return "Menengah"
    if percent_correct <= 20:
        return "Dasar"
    elif percent_correct <= 40:
        return "Pemula"
    elif percent_correct <= 60:
        return "Menengah"
    elif percent_correct <= 80:
        return "Mahir"
    else:
        return "Profesional"

"""**Ambil Learning Path**"""

def pick_learning_path(interest_cat, tech_cats):
    candidates_interest = interest_to_lp.get(interest_cat, []) if interest_cat else []
    candidates_tech = []
    for t in tech_cats:
        candidates_tech.extend(tech_to_lp.get(t, []))

    counter = Counter()
    for lp in candidates_interest:
        counter[lp] += 2
    for lp in candidates_tech:
        counter[lp] += 1

    if not counter:
        return None, []

    ranked = counter.most_common()
    top_score = ranked[0][1]
    top_lps = [lp for lp, sc in ranked if sc == top_score]

    intersection = list(set(candidates_interest) & set(candidates_tech))
    if intersection:
        inter_scores = sorted([(lp, counter[lp]) for lp in intersection], key=lambda x: x[1], reverse=True)
        chosen = inter_scores[0][0]
        return chosen, ranked

    return top_lps[0], ranked

"""**Ambil modul/tutorials**"""

# Filter modules JSON

def filter_modules_by_lp_and_level(modules_dict, learning_path_name, level):
    out = []
    if not learning_path_name or learning_path_name not in modules_dict:
        return out
    for idx, m in enumerate(modules_dict[learning_path_name]):
        lvl_field = m.get("course_level") or m.get("level") or ""
        if lvl_field.lower().startswith(level.lower()[0:3]):
            # pastikan modul punya module_id & title
            if "module_id" not in m:
                m["module_id"] = f"{learning_path_name}_{idx+1}"
            if "title" not in m:
                m["title"] = m.get("course_name") or m.get("name") or f"Module {idx+1}"
            out.append(m)
    return out

"""**Chatbot**"""

def chatbot_pipeline(user_interest_answers, user_tech_answers_mcq, student_id=None,
                     path_interest_q=PATH_INTEREST_Q, path_tech_q=PATH_TECH_Q,
                     path_modules_json=PATH_MODULES_JSON):
    interest_q_df = load_csv(path_interest_q)
    tech_q_df = load_csv(path_tech_q)
    modules = load_modules(path_modules_json)

    interest_cat, interest_scores = determine_interest_category(interest_q_df, user_interest_answers)
    tech_cats_ranked, tech_scores, percent_correct = evaluate_tech_mcq(tech_q_df, user_tech_answers_mcq)
    level = determine_json_level_from_percent(percent_correct)
    chosen_lp, lp_ranked = pick_learning_path(interest_cat, tech_cats_ranked)
    filtered_modules = filter_modules_by_lp_and_level(modules, chosen_lp, level)

    return {
        "student_id": student_id,
        "interest_category": interest_cat,
        "interest_scores": interest_scores,
        "tech_categories_ranked": tech_cats_ranked,
        "tech_scores": tech_scores,
        "tech_percent_correct": percent_correct,
        "detected_level": level,
        "chosen_learning_path": chosen_lp,
        "lp_ranked_scores": lp_ranked,
        "modules_filtered": filtered_modules
    }

"""**Roadmap**"""

LEVEL_ORDER = ["Dasar", "Pemula", "Menengah", "Mahir", "Profesional"]

def format_roadmap(res):
    learning_path_name = res["chosen_learning_path"]
    skill_level_name = res["detected_level"]
    modules = res["modules_filtered"]

    modules_list = ""
    for idx, m in enumerate(modules, start=1):
        modules_list += f"{idx}. {m['course_name']} ({m.get('course_level', '')})\n"
        for t_idx, t in enumerate(m.get("tutorials", []), start=1):
            modules_list += f"    {t_idx}. {t}\n"
        modules_list += "\n"

    try:
        current_idx = LEVEL_ORDER.index(skill_level_name)
        next_level_name = LEVEL_ORDER[current_idx + 1] if current_idx + 1 < len(LEVEL_ORDER) else "Tidak ada, ini level tertinggi ðŸŽ‰"
    except ValueError:
        next_level_name = "Level berikut tidak diketahui"

    roadmap_template = f"""
==============================
ðŸ“˜ ROADMAP PEMBELAJARAN
==============================

ðŸŽ“ Learning Path: {learning_path_name}
ðŸ“Š Skill Level: {skill_level_name}

Berikut modul yang harus kamu pelajari pada level ini:

{modules_list}
âž¡ï¸ Setelah menyelesaikan level ini, kamu bisa lanjut ke:
{next_level_name}

==============================
Cara Membaca Roadmap:
Ikuti modul berdasarkan urutan.
Selesaikan semua modul sebelum naik level.
==============================
"""
    return roadmap_template

"""**Predict - Testing**"""

if __name__ == "__main__":
    user_interest_answers = ["Mencoba membuat menu sarapan"]
    user_tech_answers_mcq = {
        "Apa yang dimaksud dengan Activity dalam pengembangan aplikasi Android?": "Komponen yang menangani tampilan pengguna",
        "Apa fungsi utama dari file AndroidManifest.xml?": "Mendefinisikan struktur dan metadata aplikasi",
        "Dalam konteks Android, apa yang dimaksud dengan Intent?": "Mekanisme untuk navigasi antar Activity"
    }

    res = chatbot_pipeline(user_interest_answers, user_tech_answers_mcq, student_id="S001")
    roadmap_text = format_roadmap(res)
    print(roadmap_text)

if __name__ == "__main__":
    user_interest_answers = ["Cari tahu masalahnya dari berbagai macam sudut pandang"]
    user_tech_answers_mcq = {
        "Apa fungsi dari cross_val_score di Scikit-learn?": "Menghitung skor validasi silang dari estimator",
        "Apa yang dimaksud dengan 'hyperparameter' dalam konteks Machine Learning?": "Jumlah total parameter dalam model",
        "Apa yang dimaksud dengan 'batch size' dalam pelatihan model Deep Learning?": "Jumlah sampel yang digunakan dalam satu iterasi update bobot",
        "Apa fungsi utama dari teknik 'pruning' dalam Decision Trees?": "Mengurangi overfitting dengan memangkas cabang yang tidak penting",
        "Apa perbedaan utama antara 'stochastic gradient descent (SGD)' dan 'mini-batch gradient descent'?": "SGD lebih cepat, mini-batch lebih akurat",
        "Apa yang dimaksud dengan 'few-shot learning' dalam konteks Machine Learning?": "Metode untuk model belajar tugas baru dengan sangat sedikit contoh berlabel"

}
    res = chatbot_pipeline(user_interest_answers, user_tech_answers_mcq, student_id="S001")
    roadmap_text = format_roadmap(res)
    print(roadmap_text)

"""BERKAS"""

import os

os.makedirs("ml/data", exist_ok=True)

!cp current_interest_question.csv ml/data/Current_Interest_Questions.csv
!cp current_tech_question.csv ml/data/Current_Tech_Questions.csv
!cp roadmap.json ml/data/modules.json

# Commented out IPython magic to ensure Python compatibility.
# # determine_interest.py
# %%writefile ml/determine_interest.py
# from collections import defaultdict
# 
# def determine_interest_category(interest_q_df, user_interest_answers):
#     opt_to_cat = dict(zip(interest_q_df["option_text"], interest_q_df["category"]))
#     scores = defaultdict(int)
#     for ans in user_interest_answers:
#         cat = opt_to_cat.get(ans)
#         if cat:
#             scores[cat] += 1
#     if not scores:
#         return None, {}
#     best = max(scores, key=scores.get)
#     return best, dict(scores)

# Commented out IPython magic to ensure Python compatibility.
# # Write evaluate_tech.py
# %%writefile ml/evaluate_tech.py
# from collections import defaultdict
# import pandas as pd
# 
# def evaluate_tech_mcq(tech_q_df, user_tech_answers):
#     tech_scores = defaultdict(int)
#     correct_count = 0
#     total_answered = 0
# 
#     for _, row in tech_q_df.iterrows():
#         q = row["question_desc"]
#         tech_cat = row.get("tech_category", "Unknown")
#         if q in user_tech_answers:
#             chosen = str(user_tech_answers[q]).strip()
#             total_answered += 1
#             if chosen and chosen == str(row["correct_answer"]).strip():
#                 tech_scores[tech_cat] += 1
#                 correct_count += 1
#             else:
#                 tech_scores[tech_cat] += 0
# 
#     percent_correct = (correct_count / total_answered * 100) if total_answered > 0 else None
#     ranked = sorted(tech_scores.items(), key=lambda x: x[1], reverse=True)
#     categories_ranked = [t for t, s in ranked]
#     return categories_ranked, dict(tech_scores), percent_correct

# Commented out IPython magic to ensure Python compatibility.
# # Write determine_level.py
# %%writefile ml/determine_level.py
# def determine_json_level_from_percent(percent_correct):
#     if percent_correct is None:
#         return "Menengah"
#     if percent_correct <= 20:
#         return "Dasar"
#     elif percent_correct <= 40:
#         return "Pemula"
#     elif percent_correct <= 60:
#         return "Menengah"
#     elif percent_correct <= 80:
#         return "Mahir"
#     else:
#         return "Profesional"

# Commented out IPython magic to ensure Python compatibility.
# # Write pick_learning_path.py
# %%writefile ml/pick_learning_path.py
# from collections import Counter
# from ml.config import interest_to_lp, tech_to_lp
# 
# def pick_learning_path(interest_cat, tech_cats):
#     candidates_interest = interest_to_lp.get(interest_cat, []) if interest_cat else []
#     candidates_tech = []
#     for t in tech_cats:
#         candidates_tech.extend(tech_to_lp.get(t, []))
# 
#     counter = Counter()
#     for lp in candidates_interest:
#         counter[lp] += 2
#     for lp in candidates_tech:
#         counter[lp] += 1
# 
#     if not counter:
#         return None, []
# 
#     ranked = counter.most_common()
#     top_score = ranked[0][1]
#     top_lps = [lp for lp, sc in ranked if sc == top_score]
# 
#     intersection = list(set(candidates_interest) & set(candidates_tech))
#     if intersection:
#         inter_scores = sorted([(lp, counter[lp]) for lp in intersection], key=lambda x: x[1], reverse=True)
#         chosen = inter_scores[0][0]
#         return chosen, ranked
# 
#     return top_lps[0], ranked

# Commented out IPython magic to ensure Python compatibility.
# # Write filter_modules.py
# %%writefile ml/filter_modules.py
# def filter_modules_by_lp_and_level(modules_dict, learning_path_name, level):
#     out = []
#     if not learning_path_name or learning_path_name not in modules_dict:
#         return out
#     for idx, m in enumerate(modules_dict[learning_path_name]):
#         lvl_field = m.get("course_level") or m.get("level") or ""
#         if lvl_field.lower().startswith(level.lower()[0:3]):
#             # pastikan modul punya module_id & title
#             if "module_id" not in m:
#                 m["module_id"] = f"{learning_path_name}_{idx+1}"
#             if "title" not in m:
#                 m["title"] = m.get("course_name") or m.get("name") or f"Module {idx+1}"
#             out.append(m)
#     return out

# Commented out IPython magic to ensure Python compatibility.
# #  Write chatbot_pipeline.py
# 
# %%writefile ml/chatbot_pipeline.py
# from ml.config import PATH_INTEREST_Q, PATH_TECH_Q, PATH_MODULES_JSON
# from ml.determine_interest import determine_interest_category
# from ml.evaluate_tech import evaluate_tech_mcq
# from ml.determine_level import determine_json_level_from_percent
# from ml.pick_learning_path import pick_learning_path
# from ml.filter_modules import filter_modules_by_lp_and_level
# 
# def chatbot_pipeline(user_interest_answers, user_tech_answers_mcq, student_id=None,
#                      path_interest_q=PATH_INTEREST_Q, path_tech_q=PATH_TECH_Q,
#                      path_modules_json=PATH_MODULES_JSON):
#     interest_q_df = load_csv(path_interest_q)
#     tech_q_df = load_csv(path_tech_q)
#     modules = load_modules(path_modules_json)
# 
#     interest_cat, interest_scores = determine_interest_category(interest_q_df, user_interest_answers)
#     tech_cats_ranked, tech_scores, percent_correct = evaluate_tech_mcq(tech_q_df, user_tech_answers_mcq)
#     level = determine_json_level_from_percent(percent_correct)
#     chosen_lp, lp_ranked = pick_learning_path(interest_cat, tech_cats_ranked)
#     filtered_modules = filter_modules_by_lp_and_level(modules, chosen_lp, level)
# 
#     return {
#         "student_id": student_id,
#         "interest_category": interest_cat,
#         "interest_scores": interest_scores,
#         "tech_categories_ranked": tech_cats_ranked,
#         "tech_scores": tech_scores,
#         "tech_percent_correct": percent_correct,
#         "detected_level": level,
#         "chosen_learning_path": chosen_lp,
#         "lp_ranked_scores": lp_ranked,
#         "modules_filtered": filtered_modules
#     }

# Commented out IPython magic to ensure Python compatibility.
# %%writefile ml/config.py
# import os
# import json
# import pandas as pd
# from collections import defaultdict, Counter
# 
# PATH_INTEREST_Q = "current_interest_question.csv"
# PATH_TECH_Q = "current_tech_question.csv"
# PATH_MODULES_JSON = "modules.json"
# 
# def load_csv(path):
#     return pd.read_csv(path)
# 
# def load_modules(path):
#     with open(path, "r", encoding="utf-8") as f:
#         return json.load(f)
# 
# interest_to_lp = {
#     "Mobile Development": ["Android Developer", "iOS Developer", "Multi-Platform App Developer"],
#     "Artificial Intelligence": ["AI Engineer", "Data Scientist", "Gen AI Engineer"],
#     "Cloud Computing": ["Google Cloud Professional", "DevOps Engineer"],
#     "Web Development": ["Front-End Web Developer", "React Developer", "Back-End Developer JavaScript"],
# }
# 
# tech_to_lp = {
#     "Android": ["Android Developer"],
#     "iOS": ["iOS Developer"],
#     "Machine Learning": ["AI Engineer", "Data Scientist"],
#     "Cloud Computing": ["Google Cloud Professional"],
#     "Web": ["Front-End Web Developer", "Back-End Developer JavaScript"],
#     "Data": ["Data Scientist"],
# }
# 
#